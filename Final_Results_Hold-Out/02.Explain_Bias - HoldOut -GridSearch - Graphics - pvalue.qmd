---
title: 
"Final File of the Explain Bias Process. 

This file works for the following resources: FB TTS, FB STT, the three telephone data sets, and any other that complies with the input file format. 

Input files must be provided in a folder named Inputs::Population data(csv), Geographical areas (Municipalities & States (shp)), Bias data (csv),Labels (csv), Optimal parameters (optional) (cvs) 

The output files are saved in the folder specified in the code as Outputs (this can be renamed by changing the value of the variable).

Data                : Facebook (TTS y STT) and the three telephone data sets. 
Validation Method   : Hold - Out
Model               : XGBOOST
graphics            : Included
Gamma               : No
Grid Search         : 5184
Last update         : 150126

"
format: html
editor: visual
---

## 1.Define / Install / Load libraries

Clear work space

```{r}
rm(list=ls())
```

```{r}
# Define packages
packages <- c(
  "xgboost",
  "parallel",
  "recommenderlab",
  "doParallel",
  "rsample",
  "tidyverse",
  "sf",
  "moments",
  "SHAPforxgboost",
  "shapviz",
  "pdp",
  "rsample",
  "foreach",
  "patchwork",
  "viridis",
  "scales",
  "tidyplots",
  "Ckmeans.1d.dp",
  "vtreat",
  "dplyr",
  "ggplot2",
  "readr",
  "ggthemes",
  "showtext",
  "Metrics",
  "caret",
  "yardstick"
)
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
} 
# Load packages
invisible(lapply(packages, library, character.only = TRUE))
```

Dependences

```{r}
#| warning: false
#| message: false
# library necessary packages
library(xgboost)
library(parallel)
library(recommenderlab)
library(doParallel)
library(rsample)
library(tidyverse)
library(sf)
library(moments)
library(SHAPforxgboost)
library(shapviz)
library(pdp)
library(rsample)
library(foreach)
library(patchwork)
library(viridis)
library(scales)
library(tidyplots)
library(Ckmeans.1d.dp)
library(vtreat)
library(dplyr)
library(readr)
library(ggthemes)
library(showtext)
library(Metrics)
library(caret)
library(yardstick)
```

Set theme

```{r}
#| include = FALSE
source("../style/data-visualisation_theme.R")
```

## 2.Parameters

```{r}
# Folder where the outputs will be saved
Outputs <- "Final_Results_HoldOut_GS5184_P05_Knn"
# Value the training set from 0 to 1
train_proportion <- 0.7
# Paths
Path01 <- "Inputs/Census_data.csv"
Path02 <- "Inputs/SHP/MunicipalitiesMX.shp"
Path03 <- "Inputs/Active_population_bias_knn_p0005.csv"
Path04 <- "Inputs/Labels.csv" 
Path05 <- "Inputs/SHP/StatesMX.shp"
# Path06 <- "Inputs/optimal_parameters.csv"

print(Path01)
print(Path02)
print(Path03)
print(Path04)
print(Path05)
# print(Path06)
```

## 3.Data

### 3.1 Population data

```{r}
Population_data <- read.csv(Path01,fileEncoding = "latin1")
colnames(Population_data)[1] <- "Area_code"
colnames(Population_data)[2] <- "Area_name"
colnames(Population_data)[3] <- "Total_Population"

# Verify Population data
head(Population_data,n=5)
# Verify missing values
print(t(t(colSums(is.na(Population_data)))))
# Verify dimensions
print(dim(Population_data))

# Population labels  ==============================================================
Population_data_labels <- names(Population_data)
```

### 3.2 Geographical areas (Municipalities) & Boundaries

```{r}
# Geographical areas ==============================================================
Geographical_areas <- st_read(Path02)
colnames(Geographical_areas)[1] <- "Area_code"
Geographical_areas <-  Geographical_areas %>% select(Area_code, geometry)

# Verify data
head(Geographical_areas,n=5)
# Verify missing values
print(t(t(colSums(is.na(Geographical_areas)))))
# Verify dimensions
print(dim(Geographical_areas))

# Boundaries ======================================================================
Boundaries <- Geographical_areas
# Simplifies the complexity of spatial geometries
Boundaries <- Boundaries %>%st_simplify(preserveTopology = TRUE, dTolerance = 1000)
# Transforms the coordinate system of spatial geometries
Boundaries <- st_transform(Boundaries,  crs = 4326)

# Verify data
head(Boundaries,n=5)
# Verify missing values
print(t(t(colSums(is.na(Boundaries)))))
# Verify dimensions
print(dim(Boundaries))
```

### 3.3 Bias data

```{r}
Bias_data <- read.csv(Path03,fileEncoding = "latin1")
colnames(Bias_data)[1] <- "Area_code"
print(head(Bias_data,n=5))
```

### 3.4 Labels

```{r}
Labels <- read.csv(Path04,fileEncoding = "latin1", header = TRUE)
colnames(Labels)[1] <- "Code_label"
colnames(Labels)[2] <- "Plot_label"
print(Labels)
```

### 3.5 Geographical areas (States)

```{r}
Geographical_areas_states <- st_read(Path05)
Geographical_areas_states$Area_code <- 1:32
colnames(Geographical_areas_states)[4] <- "Area"
Geographical_areas_states <-  Geographical_areas_states %>% 
  select(Area_code, geometry)

# Verify data
head(Geographical_areas_states,n=5)
# Verify missing values
print(t(t(colSums(is.na(Geographical_areas_states)))))
# Verify dimensions
print(dim(Geographical_areas_states))
```

### 3.6 Optimal parameters (if exists)

```{r}
# We check if the variable exists
if (exists("Path06")){
    # We check if the PATH is valid.We use file.exists() if it is a CSV file
    if (file.exists(Path06)){ 
        optimal_pars <- read.csv(Path06, fileEncoding = "latin1")
        print("Parameters loaded successfully:")
        print(optimal_pars)
    }else{
        # The variable exists, but the PATH is incorrect or the file does not exist
        print(paste("WARNING: The path",Path06," does not contain a valid file.There are no previous parameters."))
    }
}else{
    # The variable 'Path06' does NOT exist in the environment.
    print("WARNING: The variable Path06 does not exist in the environment. There are no previous parameters.")
}
```

## 4.Data wrangling

### Extract centroids

```{r}
# Calculate the centroid of each geometry within the Geographical_areas object.
centroids_df <- st_centroid(Geographical_areas) %>% 
  st_coordinates() %>% 
  as.data.frame() %>% 
  rowid_to_column("row_id")

# Combine Geographical_areas with centroids_df
centroids_with_ids <- Geographical_areas %>%
  st_set_geometry(NULL) %>% # Remove geometry to avoid issues with join
  rowid_to_column("row_id") %>% # Create a temporary row_id for merging
  cbind(centroids_df) 

# Remove row_id and rename x,y
centroids_with_ids <- centroids_with_ids %>% 
  select(-c(row_id)) %>% 
  rename(
    longitude = "X",
    latitude = "Y"
  )

# Casting "Area_code" as integer in order to use left_join
centroids_with_ids <- centroids_with_ids %>%
  mutate(Area_code = as.integer(Area_code))

# Create df_extended by join Population_data and centroids_with_ids
df_extended <- full_join(Population_data, centroids_with_ids, by = join_by(Area_code == Area_code) ) 

print(names(df_extended))
```

### Add bias_w (Add spatial weights If they are included as a feature)

```{r}
# Add spatial weights to the label data frame and df_extended if they are included 
if ("bias_w" %in% names(Bias_data)) {
  Population_data_labels <- c(Population_data_labels, "bias_w")
  df_extended <- merge(df_extended, Bias_data[, c("Area_code", "bias_w")], by = "Area_code")
}
print(Population_data_labels)
# Verify df_extended
head(df_extended,n=5)
# Verify missing values
print(t(t(colSums(is.na(df_extended)))))
# Verify dimensions
print(dim(df_extended))
```

### 4.1 Standarization

```{r}
# select numeric columns and standardize them
df_standardised <- df_extended %>%
  dplyr::select(-c(Area_code)) %>% 
  mutate(across(where(is.numeric), ~ (. - mean(.)) / sd(.))) %>%
  cbind(df_extended %>% dplyr::select(Area_code))

# Verify df_standardised
df_standardised
print(t(t(colSums(is.na(df_standardised)))))
print(dim(df_standardised))
```

### 4.2 Add bias (Add bias as dependent variable)

```{r}
input_df <- merge(df_standardised, Bias_data[, c("Area_code", "bias")], by = "Area_code")

# Verify input_df
head(input_df,n=5)
print(t(t(colSums(is.na(input_df)))))
print(dim(input_df))
```

### 4.3 Missing values

```{r}
# Check for missing values
print(t(t(colSums(is.na(input_df)))))

print(which(is.na(input_df), arr.ind = TRUE))
input_df <- input_df %>% 
  drop_na()

print(dim(input_df))
```

Removing unnecessary objects (optional)

```{r}
# rm(Geographical_areas, df_standardised,df_extended)
print("Removing unnecessary objects (optional)")
```

## 5.Setting up model

### Create folders for outputs and saving

```{r}
# 1. Create the main folder if it does not exist
if (!dir.exists(Outputs)) {
  dir.create(Outputs)
  cat(paste0("Main Folder Output '", Outputs, "' successfully created.\n"))
} else {
  cat(paste0("Main Folder Output '", Outputs, "' already exists.\n"))
}

# 2. Create the subfolders inside the main folder.
Subfolders <- c("Inputs","01.Feature Importance","02.Shap Plot","03.Shap Dependence","05.Map","Predictions")
for (Subfolder in Subfolders) {
  path_subfolder <- file.path(Outputs, Subfolder)
  if (!dir.exists(path_subfolder)) {
    dir.create(path_subfolder)
    cat(paste0("Subfolder '", Subfolder, "' created in '", Outputs, "' successfully.\n"))
  } else {
    cat(paste0("Subfolder '", Subfolder, "' already exists in '", Outputs, "'.\n"))
  }
}
cat("Folder creation process completed.\n")

# SAVING ==========================================================================
# =================================================================================
# INPUT_DF ========================================================================
# Try to write the CSV file and save the result.
write_output <- try(write.csv(input_df, file.path(Outputs, "Inputs/Input_df.csv"), fileEncoding = "latin1", row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Input_df.csv' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Input_df.csv successfully created in '", Outputs, "/Inputs\n"))
}

# POPULATION DATA =================================================================
# Try to write the CSV file and save the result.
write_output <- try(write.csv(Population_data, file.path(Outputs, "Inputs/Population_data.csv"), fileEncoding = "latin1", row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Population_data.csv' in '", Outputs, "/Inputs'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Population_data.csv successfully created in '", Outputs, "/Inputs\n"))
}

# GEOGRAPHICAL AREAS ==============================================================
# Try to write the SHP file and save the result.
write_output <- try(st_write(Geographical_areas, file.path(Outputs, "Inputs/Geographical_areas.shp"), row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Geographical_areas.shp' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Geographical_areas.shp successfully created in '", Outputs, "/Inputs\n"))
}

# BIAS DATA =======================================================================
# Try to write the CSV file and save the result.
write_output <- try(st_write(Bias_data, file.path(Outputs, "Inputs/Bias_data.csv"), fileEncoding = "latin1", row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Bias_data.csv' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Bias_data.csv successfully created in '", Outputs, "/Inputs\n"))
}

# LABELS ==========================================================================
# Try to write the CSV file and save the result.
write_output <- try(st_write(Labels, file.path(Outputs, "Inputs/Labels.csv"), fileEncoding = "latin1", row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Labels.csv' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Labels.csv successfully created in '", Outputs, "/Inputs\n"))
}

# GEOGRAPHICAL AREAS STATES =======================================================
# Try to write the SHP file and save the result.
write_output <- try(st_write(Geographical_areas_states, file.path(Outputs, "Inputs/Geographical_areas_states.shp"), row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Geographical_areas_states.shp' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Geographical_areas_states.shp successfully created in '", Outputs, "/Inputs\n"))
}
```

### 5.1 Dependent and independent variables

```{r}
# cut <- "random"

variables_to_eliminate <- c(
  "Area_code",
  "Area_name",
  "longitude", 
  "latitude",
  "bias")

print("=======================================================================")
print("DISCARDED VARIABLES:")
print("-----------------------------------------------------------------------")
print(variables_to_eliminate)

xvars <- colnames(input_df)

x <- xvars[!(xvars %in% variables_to_eliminate)]
y <- "bias"

# Independent variables
print("=======================================================================")
print("INDEPENDENT VARIABLES:")
print("-----------------------------------------------------------------------")
print(x)

# Dependent variable
print("=======================================================================")
print("DEPENDENT VARIABLES:")
print("-----------------------------------------------------------------------")
print(y)
```

### 5.2 Validation Method - Defining training and test data sets

### **Hold - Out**

```{r}
# Generating seed
set.seed(123)

# Data Division
split_df <- initial_split(input_df, prop = train_proportion)

# Get the training Set
train_sample <- training(split_df)

# Get the test Set
test_sample  <- testing(split_df)

# Verify train_sample
head(train_sample,n=5)
print(dim(train_sample))

# Verify test_sample
head(test_sample,n=5)
print(dim(test_sample))
```

### A) Treatment plan - Prepare data

```{r}
# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(train_sample,x,verbose = TRUE)

# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)  

# Prepare the training data
features_train <- vtreat::prepare(treatplan,
                                  train_sample,
                                  varRestriction = new_vars) %>% 
                                  as.matrix()

response_train <- train_sample[,y]

# Prepare the test data
features_test <- vtreat::prepare(treatplan, 
                                 test_sample, 
                                 varRestriction = new_vars) %>% 
                                 as.matrix()

response_test <- test_sample[,y]

# dimensions of one-hot encoded data
dim(features_train)
dim(features_test)

# Independent Variables
print("INDEPENDENT VARIABLES:")
print("=======================================================================")
print(colnames(features_train))
print("=======================================================================")
```

### 5.3 Tuning - Hypergrid Parameters (Only if the optimal parameters are not available)

#### 2) Parameters

```{r}
# Create hyperparameter grid

# ORIGINAL PARAMETERS
 hyper_grid_O <- expand.grid(
  #nrounds         = c(50, 100, 150),     # Number of boosting rounds
  eta              = c(.01, .05, .1, .3), # Learning rate
  #gamma           = c(0, 1, 5),          # Minimum loss reduction
  max_depth        = c(1, 3, 5, 7),       # Maximum depth of a tree
  min_child_weight = c(1, 3, 5, 7),       # Minimum sum of instance weight
  subsample        = c(.65, .8, 1),       # Fraction of rows per tree
  colsample_bytree = c(.8, .9, 1),        # Fraction of columns per tree
  lambda           = c(0, 0.5, 1),        # L2 regularization term (Ridge)
  alpha            = c(0, 0.5, 1),        # L1 regularization term (LASSO)
  optimal_trees    = 0,                   # a place to dump results
  min_RMSE         = 0                    # a place to dump results
)


# TEST PARAMETERS
 hyper_grid_T <- expand.grid(
 #nrounds          = c(50, 100, 150),     # Number of boosting rounds
 eta               = c(.01, .05),         # Learning rate
 #gamma            = c(0, 1, 5),          # Minimum loss reduction
 max_depth         = c(1, 3),             # Maximum depth of a tree
 min_child_weight  = c(1),                # Minimum sum of instance weight
 subsample         = c(.65, .8),          # Fraction of rows per tree
 colsample_bytree  = c(.8),               # Fraction of columns per tree
 lambda            = c(0, 0.5),           # L2 regularization term (Ridge)
 alpha             = c(0),           # L1 regularization term (LASSO)
 #alpha             = c(0, 0.5),           # L1 regularization term (LASSO)
 optimal_trees     = 0,                   # a place to dump results
 min_RMSE          = 0                    # a place to dump results
)

# SELECT PARAMETERS
hyper_grid <- hyper_grid_O # ORIGINAL
# hyper_grid <- hyper_grid_T # TEST

nrow(hyper_grid)

# HYPERGRID ==========================================================================
# Try to write the CSV file and save the result.
write_output <- try(write.csv(hyper_grid, file.path(Outputs, "/Hypergrid.csv"), fileEncoding = "latin1", row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Hypergrid.csv' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Hypergrid.csv successfully created in '", Outputs, "\n"))
}
```

### 5.4 Parallelised code

We apply a loop procedure to loop through and apply a XGBoost model for each hyperparameter combination and dump the results in the hyper_grid data frame. This is done in a paralellised way.

#### **Hold - Out**

```{r}
# Measure the time taken for parallel grid search
time_taken <- system.time({
    
# Set up parallel backend with the number of cores(adjust according to your system)
  num_cores <- detectCores() - 1  # to leave one core free
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)

# Parallelized grid search
    results <- foreach(
      i = 1:nrow(hyper_grid),
      .combine = rbind,
      .packages = "xgboost"
      ) %dopar% {
      # create parameter list
      params <- list(
        eta                 = hyper_grid$eta[i],
        #gamma               = hyper_grid$gamma[i],
        max_depth           = hyper_grid$max_depth[i],
        min_child_weight    = hyper_grid$min_child_weight[i],
        subsample           = hyper_grid$subsample[i],
        colsample_bytree    = hyper_grid$colsample_bytree[i],
        lambda              = hyper_grid$lambda[i],
        alpha               = hyper_grid$alpha[i]
      )

      # Create DMatrix
      dtrain_local <- xgb.DMatrix(data = features_train, label = response_train) 
      # reproducibility
      set.seed(123)

      # train model
      xgb.tune <- xgb.cv(
        params    = params,
        #data      = features_train,
        #label     = response_train,
        data      = dtrain_local,
        nrounds   = 5000,
        nfold     = 10,
        objective = "reg:squarederror", 
        verbose   = 1,                 
        early_stopping_rounds = 20,
        metrics = "rmse"
      )

      # collect the optimal number of trees and minimum RMSE
      optimal_trees <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
      min_RMSE <- min(xgb.tune$evaluation_log$test_rmse_mean)

      # return as a row (with hyperparameters and results)
      return(data.frame(
        eta              = hyper_grid$eta[i],
        #gamma            = hyper_grid$gamma[i],
        max_depth        = hyper_grid$max_depth[i],
        min_child_weight = hyper_grid$min_child_weight[i],
        subsample        = hyper_grid$subsample[i],
        colsample_bytree = hyper_grid$colsample_bytree[i],
        lambda           = hyper_grid$lambda[i],
        alpha            = hyper_grid$alpha[i],
        optimal_trees    = optimal_trees,
        min_RMSE         = min_RMSE
      ))
    }

    # Convert to a data frame if not already (should be)
    results <- as.data.frame(results)

    # Stop the cluster after processing
    stopCluster(cl)
    registerDoSEQ()  # Reset back to sequential processing

  })
  # Print the time taken
  print(time_taken)

  # Display the results
  print(results)
  
# RESULTS =========================================================================
# Try to write the CSV file and save the result.
write_output <- try(write.csv(results, file.path(Outputs, "/Results_hypergrid.csv"), fileEncoding = "latin1", row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Results_hypergrid.csv' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Results_hypergrid.csv successfully created in '", Outputs, "\n"))
}
```

Selecting optimal parameters

```{r}
results <- read_csv(file.path(Outputs, "Results_hypergrid.csv"), show_col_types = FALSE)

optimal_pars <- results %>% slice_min(order_by = min_RMSE)

# RESULTS =========================================================================
# Try to write the CSV file and save the result.
write_output <- try(write.csv(optimal_pars, file.path(Outputs, "/Optimal_parameters.csv"), fileEncoding = "latin1", row.names = FALSE))
# Check if there were any errors during writing
if (inherits(write_output, "try-error")) {
  cat(paste0("Error saving: 'Optimal_parameters.csv' in '", Outputs, "'.\n"))
  cat("Error message: ", as.character(write_output), "\n")
} else {
  cat(paste0("Optimal_parameters.csv successfully created in '", Outputs, "\n"))
}
print(optimal_pars)
```

## 6.Final model

Once you've found the optimal model, we can fit our final model

### 6.1 Fit final model

```{r}
# parameter list
  params <- list(
    eta              = optimal_pars$eta,
    #gamma            = optimal_pars$gamma,
    max_depth        = optimal_pars$max_depth,
    min_child_weight = optimal_pars$min_child_weight,
    subsample        = optimal_pars$subsample,
    colsample_bytree = optimal_pars$colsample_bytree,
    lambda           = optimal_pars$lambda,
    alpha            = optimal_pars$alpha,
    objective        = "reg:squarederror",
    verbose          = 0
  )

# Optimal trees
  optimal_nrounds <- optimal_pars$optimal_trees
  
# dtrain
  dtrain <- xgb.DMatrix(data = features_train, label = response_train)
  
# train final model
    #xgb.fit.final <- xgboost(
    xgb.fit.final <- xgb.train(
    params    = params,
    data      = dtrain,
    #data      = features_train,
    #label     = response_train,
    nrounds   = optimal_nrounds
    )

print(paste("Final model trained with", optimal_nrounds, "trees."))

xgb.fit.final$feature_names
# Save optimal model
saveRDS(xgb.fit.final, file.path(Outputs, "xgb-fit-final-model.rds"))
```

### 6.2 Predictions XGBoost

```{r}
# ============================ TRAINING SAMPLE ====================================
# SET NAMES
aux_train_sample <- setNames(train_sample[, x], xgb.fit.final$feature_names)

# DROP NA NAMES 
aux_train_sample <- aux_train_sample[, !is.na(names(aux_train_sample))]

# PREDICTION IN TRAINING DATA
train_sample$bias_prediction <- predict(xgb.fit.final,as.matrix(aux_train_sample))

# RMSE
train_sample_rmse <- sqrt(sum((train_sample$bias - train_sample$bias_prediction)^2 )/nrow(train_sample))

# PEARSON CORRELATION WITH P-VALUE
pearson_test_train <- cor.test(train_sample$bias, 
                                train_sample$bias_prediction, 
                                method = "pearson")
train_sample_pearson_cor <- pearson_test_train$estimate
train_sample_pearson_p   <- pearson_test_train$p.value
  
# SPEARMAN CORRELATION WITH P-VALUE
spearman_test_train <- cor.test(train_sample$bias, 
                                train_sample$bias_prediction, 
                                method = "spearman")
train_sample_spearman_cor <- spearman_test_train$estimate
train_sample_spearman_p   <- spearman_test_train$p.value

# STANDAR DEVIATION
train_sample_sd_error <- sd(train_sample$bias - train_sample$bias_prediction)

# SSE
train_sse <- sum((train_sample$bias - train_sample$bias_prediction)^2)
# SST
train_sst <- sum((train_sample$bias - mean(train_sample$bias))^2)
# R^2
train_sample_r_squared <- 1 - (train_sse / train_sst)


# ============================ TEST SAMPLE =======================================
# SET NAMES
aux_test_sample <- setNames(test_sample[, x], xgb.fit.final$feature_names)

# DROP NA NAMES 
aux_test_sample <- aux_test_sample

# PREDICTION IN TEST DATA
test_sample$bias_prediction <- predict(xgb.fit.final,as.matrix(aux_test_sample))

# RMSE
test_sample_rmse <- sqrt( sum( (test_sample$bias - test_sample$bias_prediction)^2 )/nrow(test_sample))

# PEARSON CORRELATION WITH P-VALUE
pearson_test_eval <- cor.test(test_sample$bias, 
                              test_sample$bias_prediction, 
                              method = "pearson")
test_sample_pearson_cor <- pearson_test_eval$estimate
test_sample_pearson_p   <- pearson_test_eval$p.value

# SPEARMAN CORRELATION WITH P-VALUE
spearman_test_eval <- cor.test(test_sample$bias, 
                              test_sample$bias_prediction, 
                              method = "spearman")
test_sample_spearman_cor <- spearman_test_eval$estimate
test_sample_spearman_p   <- spearman_test_eval$p.value

# STANDAR DEVIATION
test_sample_sd_error <- sd(test_sample$bias - test_sample$bias_prediction)

# SSE
test_sse <- sum((test_sample$bias - test_sample$bias_prediction)^2)
# SST
test_sst <- sum((test_sample$bias - mean(test_sample$bias))^2)
# R^2
test_sample_r_squared <- 1 - (test_sse / test_sst)

# ============================ SAVING PREDICTIONS =================================
# --- Save TRAINING ---
filename_train  <- ("Predictions/Predictions_train.csv")
file_path_train <- file.path(Outputs,filename_train)
write_output_train<-try(readr::write_csv(train_sample,file=file_path_train))
if (inherits(write_output_train, "try-error")){
    cat(paste0("Error saving Predictions_train.csv"))
}else{
    cat(paste0(filename_train, " successfully created with attributes.\n"))
}
  
# --- Save TEST ---
filename_test <- paste0("Predictions/Predictions_test.csv")
file_path_test <- file.path(Outputs, filename_test)
write_output_test<-try(readr::write_csv(test_sample, file = file_path_test))
if (inherits(write_output_test, "try-error")) {
    cat(paste0("Error saving Predictions_test.csv"))
}else{
    cat(paste0(filename_test, " successfully created with attributes.\n"))
}
```

### 6.3 MÃ©trics table

```{r}
train_sample_metrics <- c(train_sample_rmse,
                          train_sample_pearson_cor,
                          train_sample_pearson_p,
                          train_sample_spearman_cor, 
                          train_sample_spearman_p,
                          train_sample_sd_error,
                          train_sample_r_squared) %>% round(3)
                          

test_sample_metrics <- c(test_sample_rmse,
                         test_sample_pearson_cor,
                         test_sample_pearson_p,
                         test_sample_spearman_cor, 
                         test_sample_spearman_p,
                         test_sample_sd_error,
                         test_sample_r_squared) %>% round(3)

    metric <- c(
      "RMSE",
      "Pearson",
      "P_Value_Pearson", 
      "Spearman", 
      "P_Value_Spearman", 
      "SD_Error",
      "R2")

summary_table <- data.frame(metric, train_sample_metrics, test_sample_metrics) 
names(summary_table) <- c("Metric", "Train", "Test")
summary_table

# Saving model performance
write_csv(summary_table, file.path(Outputs, "Model-performance.csv"))
print("Training and predictions of model completed")

print(summary_table)
```

## 7.Graphics

### 7.1 Feature_Importance_PDF

```{r}
library(shapviz)
library(dplyr)
library(ggplot2)
library(scales)
library(showtext)

# Font
font <- "sans"
# font <- "Fira Sans"
# font <- "Source Sans Pro"
tamanio <- 17

# ---------------------------------------------------------------------------------
# 0. PREPARATION
# ---------------------------------------------------------------------------------
# 1. Load the final trained model
model_filename <- "xgb-fit-final-model.rds"
file_path <- file.path(Outputs, model_filename)
  
# Handling the error if the model does not exist
xgb.fit.final <- tryCatch({
    readRDS(file_path)
}, error = function(e) {
    cat(paste0("Error: Could not load model"))
    return(NULL)
})
# ---------------------------------------------------------------------------------
# 1.UNIQUE SHAP CALCULATION
# ---------------------------------------------------------------------------------
shp <- shapviz(
    xgb.fit.final, 
    X_pred = features_train, 
    X = features_train      
)
## print(shp)
## print(names(shp$X))
# ---------------------------------------------------------------------------------
# 2.DATA EXTRACTION, LABELING AND PREPARATION
# ---------------------------------------------------------------------------------
importance_data <- shapviz::sv_importance(shp, kind = "bar", max_display = 20L) %>%
    .$data %>% 
    dplyr::left_join(Labels, by = c("feature" = "Code_label")) %>% 
    dplyr::select(feature = Plot_label, value)

## print(importance_data)
importance_data$feature <- factor(importance_data$feature, levels = rev(importance_data$feature))

## print(importance_data)
# ---------------------------------------------------------------------------------
# 3.GRAPH GENERATION
# ---------------------------------------------------------------------------------
Plot <- ggplot(importance_data, aes(x = value, y = feature))+
        geom_segment(aes(x = 0, 
                         xend = value),
                     color = "#000000", 
                     linewidth = 0.5, 
                     linetype = "solid") +
        geom_point(aes(color = value > 0), 
                   size = 7, 
                   alpha = 1) +
        scale_color_manual(values = c("TRUE" = "#800080", "FALSE" = "#800080")) +
        labs(
          title = "A) Feature importance",
          x     = "Impact SHAP Value",
          y     = NULL)+
        theme_classic()+
        scale_x_continuous(
          limits = c(0, max(importance_data$value) * 1.05), 
          expand = c(0, 0))+
        theme(
          plot.title = element_text(size   = tamanio, 
                                face   = "bold", 
                                hjust  = 0, 
                                color  = "#000000", 
                                family = font),
          plot.title.position = "plot",
          axis.title.x = element_text(size = tamanio),
          axis.text.x=element_text(size = tamanio,color = "#000000",family = font),
          axis.text.y=element_text(size = tamanio,color = "#000000",family = font),
          legend.position = "none",
          plot.margin = margin(5, 20, 5, 5)
    )
print(Plot)
# ---------------------------------------------------------------------------------
# 4. SAVING
# ---------------------------------------------------------------------------------
# CSV
importance_data_for_csv <- importance_data %>%
      dplyr::select(
        Feature_Label = feature,
        Impact_SHAP_Value = value)

csv_file_name<- "01.Feature Importance/Feature_Importance.csv"
csv_full_path <- file.path(Outputs, csv_file_name)
write.csv(importance_data_for_csv, csv_full_path, row.names = FALSE)
message(paste("Saved data to CSV:", csv_full_path))

# PDF
file_name <- paste0("01.Feature Importance/Feature_Importance.pdf")
full_path <- file.path(Outputs, file_name)
ggsave(
    full_path,  
    plot     = Plot,
    device = cairo_pdf,
    width    = 7, 
    height   = 7, 
    units    = "in")
message(paste("Saved graphic:", full_path))
```

### 7.2.Shap plot

```{r}
library(shapviz)
library(dplyr)
library(ggplot2)
library(scales)
library(showtext)

# Font
font <- "sans"
# font <- "Fira Sans"
# font <- "Source Sans Pro"
tamanio <- 17
# ---------------------------------------------------------------------------------
# 0. PREPARATION
# ---------------------------------------------------------------------------------
# 1. Load the final trained model
model_filename <- "xgb-fit-final-model.rds"
file_path <- file.path(Outputs, model_filename)
  
# Handling the error if the model does not exist
xgb.fit.final <- tryCatch({
    readRDS(file_path)
}, error = function(e) {
    cat(paste0("Error: Could not load model"))
    return(NULL)
})
# ---------------------------------------------------------------------------------
# 1.UNIQUE SHAP CALCULATION
# ---------------------------------------------------------------------------------
shp <- shapviz(
    xgb.fit.final, 
    X_pred = features_train, 
    X = features_train      
)
## print(shp)
## print(names(shp$X))

  importance_ranking <- shapviz::sv_importance(
    shp, 
    kind = "bar", 
    max_display = 20L)%>%
    .$data %>%
    dplyr::arrange(desc(value))
  
  top_n_features <- importance_ranking %>%
    dplyr::slice_head(n = 20L) %>%
    dplyr::pull(feature)
# ---------------------------------------------------------------------------------
# 2.DATA EXTRACTION, LABELING AND ORDERING
# ---------------------------------------------------------------------------------
# Extract the most important features
importance_data <- importance_ranking %>%
    dplyr::left_join(Labels, by = c("feature" = "Code_label")) %>%
    dplyr::select(feature = Plot_label)
  
# Factor the features column to ensure order
importance_data$feature <- factor(importance_data$feature, 
                                 levels = rev(importance_data$feature))

## print(importance_data)
# ---------------------------------------------------------------------------------
# 2.5.CREATE LONG SHAPS
# ---------------------------------------------------------------------------------
# 4. Generate the SHAP Long dataset
shap_long <- shap.prep(xgb_model = xgb.fit.final, X_train = features_train)

  #1.1 Area Code
  area_codes_key <- train_sample %>%
  dplyr::mutate(Area_code = as.integer(Area_code)) %>% 
  sf::st_drop_geometry() %>% 
  dplyr::select(Area_code) %>%
  dplyr::mutate(ID = row_number()) 
  
  selected_data <- shap_long %>%
  dplyr::filter(variable %in% top_n_features) %>% 
  dplyr::left_join(area_codes_key, by = "ID") %>% 
  dplyr::left_join(Labels, by = c("variable" = "Code_label")) %>%
  dplyr::select(-ID)
  
# ---------------------------------------------------------------------------------
# 3. SAVING CSV
# ---------------------------------------------------------------------------------
 shap_data_for_csv <- selected_data %>%
      dplyr::select(
        Area_code, 
        Feature = variable, 
        Feature_Label = Plot_label,
        Feature_Value = rfvalue, 
        SHAP_Value = value)
  
csv_file_name <- "02.Shap Plot/Shap_Plot.csv"
csv_full_path <- file.path(Outputs, csv_file_name)
write.csv(shap_data_for_csv, csv_full_path, row.names = FALSE)
message(paste("Saved SHAP Data to CSV:", csv_full_path))
# ---------------------------------------------------------------------------------
# 3.GRAPH GENERATION
# ---------------------------------------------------------------------------------
Plot <- shapviz::sv_importance(
  shp,
  kind = "bee",
  max_display = 20L,
  viridis_args = list(option = "A"), # "inferno"
  bee_width = 0.2)+ # Adjust the spread of the points
  
  # Y-axis Label Adjustment
  scale_y_discrete(labels = levels(importance_data$feature)) +
    
  # X-Axis Adjustment
  scale_x_continuous(
        breaks = scales::pretty_breaks(n = 5))+
  # Title
  ggtitle("B) SHAP Values") +

  # Custom Styles
  theme(
      axis.title.x = element_text(size = tamanio, family = font, face = "plain"),
      axis.text.y = element_text(size = tamanio, color = "#000000", family = font),
      axis.text.x = element_text(size = (tamanio - 2), color = "#000000", family = font),
      legend.title = element_text(size = tamanio, family = font, face = "plain"),
      legend.text = element_text(size = tamanio, family = font, face = "plain"),
  # Grid lines
      panel.grid.major.x = element_line(color = "#e0e0e0", linetype = "dotted"),
  # Axes and Margins
      axis.line = element_line(color = "black", linewidth = 0.8),
      plot.title=element_text(size=tamanio,family=font, hjust = 0, face = "bold"),
      plot.title.position = "plot",
      panel.background = element_rect(fill = "transparent")
    )

print(Plot)
# ---------------------------------------------------------------------------------
# 4. SAVING
# ---------------------------------------------------------------------------------
file_name <- "02.Shap Plot/Shap_Plot.pdf"
full_path <- file.path(Outputs, file_name)
ggsave(
    full_path, 
    plot     = Plot, 
    device = cairo_pdf,
    width    = 7, 
    height   = 7, 
    units = "in", 
    bg = "white") 
message(paste("Saved graphic:", full_path))
```

### 7.3.Shap Dependence

```{r}
library(shapviz)
library(dplyr)
library(ggplot2)
library(scales)
library(stringr)

# Font
font <- "sans"
# font <- "Fira Sans"
# font <- "Source Sans Pro"
tamanio <- 17
top_n <- 7
# ---------------------------------------------------------------------------------
# 0. PREPARATION
# ---------------------------------------------------------------------------------
# 1. Load the final trained model
model_filename <- "xgb-fit-final-model.rds"
file_path <- file.path(Outputs, model_filename)
  
# Handling the error if the model does not exist
xgb.fit.final <- tryCatch({
    readRDS(file_path)
}, error = function(e) {
    cat(paste0("Error: Could not load model"))
    return(NULL)
})
# ---------------------------------------------------------------------------------
# 1.UNIQUE SHAP CALCULATION
# ---------------------------------------------------------------------------------
shp <- shapviz(
    xgb.fit.final, 
    X_pred = features_train, 
    X = features_train      
)
## print(shp)
## print(names(shp$X))

# ---------------------------------------------------------------------------------
# 2.IDENTIFICATION OF TOP N FEATURES
# ---------------------------------------------------------------------------------
# 3. Extract the n most important features (by absolute mean SHAP value)
top_n_features <- shapviz::sv_importance(shp, kind = "bee", max_display = top_n) %>%
    .$data %>%
    dplyr::distinct(feature) %>% 
    dplyr::pull(feature) # Extract the code vector
  
# ---------------------------------------------------------------------------------
# 2.CREATE LONG SHAPS
# ---------------------------------------------------------------------------------
# 4. Generate the SHAP Long dataset
shap_long <- shap.prep(xgb_model = xgb.fit.final, X_train = features_train)

  #1.1 Area Code
  area_codes_key <- train_sample %>%
  dplyr::mutate(Area_code = as.integer(Area_code)) %>% 
  sf::st_drop_geometry() %>% 
  dplyr::select(Area_code) %>%
  dplyr::mutate(ID = row_number()) 
  
  selected_data <- shap_long %>%
  dplyr::filter(variable %in% top_n_features) %>% 
  dplyr::left_join(area_codes_key, by = "ID") %>% 
  dplyr::left_join(Labels, by = c("variable" = "Code_label")) %>%
  dplyr::select(-ID)


# The list of variables for the loop:
selected_variables <- unique(selected_data$variable)
# ---------------------------------------------------------------------------------
# 3. SAVING CSV
# ---------------------------------------------------------------------------------
 shap_data_for_csv <- selected_data %>%
      dplyr::select(
        Area_code, 
        Feature = variable, 
        Feature_Label = Plot_label,
        Feature_Value = rfvalue, 
        SHAP_Value = value)
  
csv_file_name <- "03.Shap Dependence/SHAP_Dependence.csv"
csv_full_path <- file.path(Outputs, csv_file_name)
write.csv(shap_data_for_csv, csv_full_path, row.names = FALSE)
message(paste("Saved SHAP Dependence Data to CSV:", csv_full_path))
# ---------------------------------------------------------------------------------
# 3.GRAPH GENERATION
# ---------------------------------------------------------------------------------
for (var_code in selected_variables) {
  # 5. Filter data for the current variable
  data_subset <- selected_data %>% dplyr::filter(variable == var_code)
    
  # 6. Obtain legible title (Labeling)
  plot_title_label <- Labels %>%
      dplyr::filter(Code_label == var_code) %>%
      dplyr::pull(Plot_label)
    
  # If the label is not found, use the code
  if (length(plot_title_label) == 0) {
      plot_title_label <- var_code}
    
  # 7. Generation of the Dependency Graph
  Plot <- ggplot(data_subset, aes(x = rfvalue, y = value))+
    
    # Points (SHAP value vs. Feature value)
    geom_point(aes(color = rfvalue), alpha = 0.5, size = 3)+
    
    # Trend Line
    geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), 
                    se = TRUE, fill = "grey90", color = "black", linewidth = 1.2)+
    
    # Styles and Scales
    scale_color_viridis_c(option = "inferno") +
    scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale(), accuracy = 0.001)) +
        
    # Titles
    labs(
        #title = plot_title_label, 
        x = "Feature Value", 
        y = "SHAP Value") +
    theme_classic()+
        
    # Custom Styles
    theme(
#plot.title = element_text(size = 20, family = font, face = "bold", hjust = 0)
axis.title.x = element_text(family = font, size = 25, color = "black", margin = margin(t = 10)),
axis.title.y = element_text(family = font, size = 25, color = "black", margin = margin(r = 10)),
axis.text.x  = element_text(family = font, size = 17, color = "black"),
axis.text.y  = element_text(family = font, size = 17, color = "black"),

legend.position = "none",
axis.line = element_line(color = "black", linewidth = 0.8),
plot.margin = margin(10, 20, 10, 10)
)
  
print(Plot)
# ---------------------------------------------------------------------------------
# 4. SAVING
# ---------------------------------------------------------------------------------
# PDF
sanitized_title <- str_replace_all(plot_title_label, "[^a-zA-Z0-9_.]", "_")
file_name <- paste0("03.Shap Dependence/Shap_Dependence_", sanitized_title, ".pdf")
full_path <- file.path(Outputs, file_name)  
ggsave(full_path, 
       plot   = Plot, 
       device = cairo_pdf,
       width  = 10, 
       height = 7, 
       units = "in", 
       bg = "white") 
message(paste("Saved graphic:", full_path))
}
```

### 7.4.Heatmap

### 7.5.Map

```{r}
library(shapviz)
library(dplyr)
library(tidyr)
library(ggplot2)
library(sf)
library(stringr)
library(units)

# Font
font <- "sans"
tamanio <- 15
# font <- "Fira Sans"
# font <- "Source Sans Pro"
top_n <- 7
Tema <- "mako"

# ---------------------------------------------------------------------------------
# 0. SPATIAL PREPARATION
# ---------------------------------------------------------------------------------
# Municipalities
Boundaries <- Geographical_areas %>%
  st_transform(crs = 6372) %>%
  st_simplify(preserveTopology = TRUE, dTolerance = 1000) %>%
  st_make_valid() %>% 
  mutate(Area_code = as.integer(Area_code))
  
# States
state_boundaries <- Geographical_areas_states %>%
  st_transform(crs = 6372) %>%
  st_simplify(preserveTopology = TRUE, dTolerance = 1000) %>%
  st_make_valid() %>% 
  mutate(Area_code = as.integer(Area_code))

# ---------------------------------------------------------------------------------
# 1. PREPARATION
# ---------------------------------------------------------------------------------
# 1. Load the final trained model
model_filename <- "xgb-fit-final-model.rds"
file_path <- file.path(Outputs, model_filename)
  
# Handling the error if the model does not exist
xgb.fit.final <- tryCatch({
    readRDS(file_path)
}, error = function(e) {
    cat(paste0("Error: Could not load model"))
    return(NULL)
})
# ---------------------------------------------------------------------------------
## 2. RAW SHAP CALCULATION AND DATA EXTRACTION
# ---------------------------------------------------------------------------------
  # Calculate the raw SHAP values (Matrix) and the SHAP object
  shp <- shapviz::shapviz(
    xgb.fit.final, 
    X_pred = features_train, 
    X = features_train)
  
  shap_values_raw <- shap.values(
    xgb_model = xgb.fit.final, 
    X_train = features_train)
  
  aux_shap_scores <- as.data.frame(shap_values_raw$shap_score)
    
  # Extract Area_code and join (using the unprocessed train_sample)
  area_codes_df <- train_sample %>%
    sf::st_drop_geometry() %>% 
    dplyr::select(Area_code)
  aux_wide <- cbind(area_codes_df, aux_shap_scores) 
  
# ---------------------------------------------------------------------------------
# 3. GENERATION OF THE LONG DATAFRAME FOR MAPPING
# ---------------------------------------------------------------------------------   # Generate the input_df_with_shap_values for the Fold k
  input_df_with_shap_values <- aux_wide %>%
    tidyr::pivot_longer(
        cols = -Area_code,
        names_to = "feature",
        values_to = "shap_value"
    ) %>%
    dplyr::left_join(Labels, by = c("feature" = "Code_label")) %>%
    dplyr::select(Area_code, feature, Plot_label,shap_value)
# ---------------------------------------------------------------------------------
# 4. IDENTIFICATION OF TOP N FEATURES
# ---------------------------------------------------------------------------------
# Extract the codes of the Top N most important features (by their absolute mean SHAP value)
top_n_features_codes <- shapviz::sv_importance(shp, kind = "bee", max_display = top_n) %>%
  .$data %>%
  dplyr::distinct(feature) %>%
  dplyr::pull(feature)

# Label Preparation and Mapping
top_n_labels_map <- Labels %>%
  dplyr::filter(Code_label %in% top_n_features_codes) %>%
  dplyr::select(Code_label, Plot_label)

# ---------------------------------------------------------------------------------
# 5. SAVING CSV
# ---------------------------------------------------------------------------------
  shap_data_for_csv <- input_df_with_shap_values %>%
      dplyr::filter(feature %in% top_n_features_codes)%>%
      dplyr::select(
        Area_code, 
        Feature = feature, 
        Plot_label,
        SHAP_Value = shap_value)
  
  csv_file_name <- paste0("05.Map/Map.csv")
  csv_full_path <- file.path(Outputs, csv_file_name)
  write.csv(shap_data_for_csv,csv_full_path, row.names = FALSE)
  message(paste("Saved SHAP Map Data to CSV:", csv_full_path))
# ---------------------------------------------------------------------------------
# 6.GRAPH GENERATION
# ---------------------------------------------------------------------------------
for (feature_code in top_n_features_codes){
    
    # Get the map title
    plot_title_label <- top_n_labels_map %>%
      dplyr::filter(Code_label == feature_code) %>%
      dplyr::pull(Plot_label)
      
    # Prepare SHAP data: Filter the SHAP values dataframe
    shap_data_for_map <- input_df_with_shap_values %>%
      dplyr::filter(feature == feature_code) %>%
      dplyr::select(Area_code, shap_value)
    
    # Join the geometries with the SHAP values of the current feature
    map_data <- Boundaries %>%
      dplyr::left_join(shap_data_for_map, by = "Area_code")
      
    # Creation of the Choropleth Map
    Plot <- ggplot(data = map_data)+
    
    # Municipalities layer: Filled with SHAP values
    geom_sf(aes(fill = shap_value), color = NA, linewidth = 0.1) +
      
    # State layer: contours
    geom_sf(data = state_boundaries, fill = NA, color = "black", linewidth = 0.1) +
      
    # Stylized
    scale_fill_viridis_c(option = Tema, name = "SHAP Value", na.value = "grey80") +
      theme_void()+
      
    # Custom Styles
    theme(
        legend.position = c(0.0, 0.2),
        legend.justification = "left",
        legend.direction = "horizontal",
        legend.title = element_text(family = font, size = tamanio, face = "plain", hjust = 0.5),
        legend.text = element_text(family = font, size = 12),
        legend.key.width = unit(1.4, "cm"),
        legend.key.height = unit(0.5, "cm"),
        plot.margin = margin(10, 10, 10, 10)
      )
    
print(Plot)

# ---------------------------------------------------------------------------------
# 7. SAVING
# ---------------------------------------------------------------------------------
# PDF
sanitized_title <- str_replace_all(plot_title_label, "[^a-zA-Z0-9_.]", "_")
file_name <- paste0("05.Map/Map_", sanitized_title, ".pdf")
full_path <- file.path(Outputs, file_name)  
ggsave(full_path, 
       plot   = Plot, 
       device = cairo_pdf,
       width  = 10, 
       height = 7, 
       units = "in", 
       bg = "white") 
message(paste("Saved graphic:", full_path))
}
```
